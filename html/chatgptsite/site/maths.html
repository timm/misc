<!DOCTYPE html>
<html lang="en">
<head>
<link href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism.css" rel="stylesheet" />
<script src="https://cdn.jsdelivr.net/npm/prismjs/prism.js"></script>
<style>
code, pre {
  font-family: "Fira Code", "Source Code Pro", "Menlo", "Consolas", monospace;
  font-size: 0.95em;
  background-color: #f5f5f5;
  padding: 0.5em;
  border-radius: 6px;
  line-height: 1.5;
}
</style>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kube.py - Mathematical Foundations</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div class="menu">
            <small><a href="index.html">Home</a> | <a href="why.html">Why</a> | <a href="eg.html">Eg</a> | <a href="maths.html">Maths</a> | <a href="what.html">What</a> | <a href="how.html">How</a> | <a href="who.html">Who</a> | <a href="details.html">Details</a></small>
        </div>
        <hr>
    </header>

    <div class="github-corner">
        <a href="https://github.com/timm/kube" aria-label="Fork me on GitHub">
            <svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
                <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
                <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
                <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
            </svg>
        </a>
    </div>

    <main>
        <h1>Mathematical and AI Foundations</h1>

        <h2>Minkowski Distance</h2>

        <p>Kube uses the Minkowski distance metric as the foundation for measuring 
        similarity between data points. The formula is:</p>

<pre><code class="language-html">d(x, y) = (Σ|x_i - y_i|^P)^(1/P)</code></pre>

        <p>Where:</p>
        <ul>
            <li>x and y are two data points</li>
            <li>n is the number of dimensions</li>
            <li>P is the distance formula exponent (default: 2)</li>
        </ul>

        <p>When P=2, this becomes the familiar Euclidean distance. Different P values 
        create different distance spaces, affecting how clusters form.[21]</p>

        <h2>Locality-Sensitive Hashing (LSH)</h2>

        <p>Kube implements a form of locality-sensitive hashing for efficient clustering. 
        LSH works by:</p>

        <ol>
            <li>Selecting representative "poles" from the data</li>
            <li>Projecting each data point onto the lines connecting these poles</li>
            <li>Creating a discrete hash based on these projections</li>
            <li>Grouping points with identical hashes</li>
        </ol>

        <p>This approach has O(n) complexity versus the O(n²) of traditional clustering 
        methods, making it efficient for large datasets.[22]</p>

        <h2>Active Learning</h2>

        <p>The code implements active learning principles by finding the most informative 
        examples in the data space. The <code>poles()</code> method identifies data points at 
        maximum distances from each other, effectively sampling the boundaries of the 
        data space.[23]</p>

        <p>Active learning focuses on selecting the most informative examples for labeling, 
        rather than randomly sampling the data space. This is particularly useful in domains 
        where labeling is expensive, as it maximizes the information gain from each labeled 
        example.</p>

        <h2>Multi-Objective Optimization</h2>

        <p>Kube optimizes for multiple objectives through the "heaven" concept. For each 
        column marked with "+" or "-" in the CSV header, kube attempts to maximize or 
        minimize those values, respectively. The <code>ydist()</code> method calculates how far 
        each data point is from the ideal point ("heaven").[24]</p>

        <p>In multi-objective optimization, there is rarely a single solution that optimizes all 
        objectives simultaneously. Instead, we seek Pareto-optimal solutions - those where improving 
        one objective necessarily worsens another. Kube's approach combines multiple objectives into 
        a single distance metric, allowing for intuitive ranking of solutions.</p>

        <h2>Dimensionality Reduction</h2>

        <p>At its core, kube implements a form of dimensionality reduction through projection. This 
        is related to the Johnson-Lindenstrauss lemma[15], which proves that high-dimensional data can 
        be projected into significantly lower dimensions while mostly preserving distances between points.</p>

        <p>By representing complex, high-dimensional data in a lower-dimensional space, kube makes it 
        possible to visualize and understand relationships that would otherwise be difficult to perceive.</p>
    </main>

    <footer>
        <hr>
        <small>&copy; 2025 Tim Menzies. MIT License.</small>
    </footer>
</body>
</html>
