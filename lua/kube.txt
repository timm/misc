kube.py

(c) 2025, Tim Menzies <timm@ieee.org>, MIT License

Simplicity is the ultimate sophistication.
Small data, tiny code, explainable multi-objective AI.

Options:
      -b bins    number of bins                     = 5
      -m min     minPts per cluster (0=auto choose) = 0
      -P P       distance formula exponent          = 2
      -d dims    number of dimensions               = 4
      -r rseed   random number seed                 = 1234567891
      -s some    search space size for poles        = 30
      -f file    training csv file = ../../moot/optimize/misc/auto93.csv

Kube.py is a Python implementation for explainable AI (XAI) focused on active
learning and multi-objective optimization. Created by Tim Menzies in 2025, this
lightweight framework provides powerful clustering, projection, and optimization
capabilities with minimal dependencies.

The name "kube" (from the Greek "κύβος" meaning "cube") suggests the
multi-dimensional space handling capabilities of this tool. Its primary purpose
is to help navigate complex data spaces efficiently while keeping explanations
transparent and interpretable.

-------------------------------------------------------------------------------

Practical Example

Let's start with a practical example of how kube.py can be used to analyze and
cluster automobile data.

Input Data Format

Kube.py works with CSV data where the column headers contain important metadata.
Here's a sample of the auto93.csv dataset:

 Clndrs  Volume  HpX  Model  origin  Lbs-  Acc+  Mpg+  ydist
------  ------  ---  -----  ------  ----  ----  ----  -----
4       90      48   80     2       2085  21.7  40    0.17
4       91      67   80     3       1850  13.8  40    0.17
4       86      65   80     3       2019  16.4  40    0.17
4       86      64   81     1       1875  16.4  40    0.18
...
8       429     198  70     1       4341  10.0  20    0.77
8       350     165  70     1       3693  11.5  20    0.77
8       455     225  70     1       4425  10.0  10    0.77
8       454     220  70     1       4354  9.0   10    0.79

Column Naming Conventions

- Capitalized headers (like "Clndrs", "Volume") indicate numeric columns
- Lowercase headers (like "origin") indicate symbolic/categorical columns
- A trailing "-" (like "Lbs-") indicates values to be minimized
- A trailing "+" (like "Acc+", "Mpg+") indicates values to be maximized
- A trailing "X" (not shown) would indicate columns to be ignored

Data Interpretation

- The example shows cars with different attributes (cylinders, volume,
horsepower, etc.)
- Lower rows show larger cars (8 cylinders, high weight) with poor fuel
economy (10-20 MPG)
- Upper rows show smaller cars (4 cylinders, lower weight) with better fuel
economy (40 MPG)
- The "ydist" column (distance to heaven) shows that the top cars (0.17) are
much closer to ideal than bottom cars (0.79)
- The "origin" column uses codes (1=American, 2=European, 3=Japanese)

Running the Example

$ python kube.py --counts

# Results might look like:
# {mid: 0.29, div: 0.11, n: 8}
# {mid: 0.45, div: 0.15, n: 12}
# {mid: 0.19, div: 0.08, n: 10}
# ...

What Happened?

In this example, kube.py has:
1. Loaded the auto93.csv dataset
2. Selected representative "poles" from the data
3. Used locality-sensitive hashing (LSH) to cluster similar items
4. Computed statistics for each cluster
5. Displayed clusters with their central tendency (mid), diversity (div),
and size (n)

Each cluster represents automobiles with similar characteristics, and the "mid"
value represents how close each cluster is to an ideal car (lower is better).
This demonstrates kube's ability to automatically organize complex
multi-dimensional data into meaningful groups.

Try More Examples

Explore other example functions to understand different capabilities:

# Print the current configuration
$ python kube.py --the

# See how kube understands your columns
$ python kube.py --data

# Find the best and worst examples in your data
$ python kube.py --ydist

# See the clustering structure
$ python kube.py --poles

-------------------------------------------------------------------------------

The Philosophy of Lightweight AI

In today's AI-powered age, the tools are dazzling. Code writes itself. Systems
configure themselves. Everything is faster, easier, more accessible. But there's
a cost to this convenience – we risk forgetting how the machine actually works.

Kube.py represents a counterbalance to the prevailing trend of ever-larger AI
models. It embodies what some call "Small AI" – lightweight, explainable
approaches that achieve remarkable results with minimal computational resources.

Why seek such alternatives? Shouldn't "Big AI," with its massive datasets and
CPU-intensive methods, be the undisputed champion? Not necessarily. In multiple
studies, simpler methods have outperformed complex Large Language Models,
running significantly faster and often producing more accurate results.
Astonishingly, many research papers on large models don't even benchmark
against these more straightforward alternatives.

Compelling Reasons for Lightweight AI

1. Engineering Elegance & Cost: There's inherent satisfaction in achieving more
   with less. If everyone else can do it for $100, wouldn't you want to do it
   for one cent? Kube.py demonstrates this principle by implementing
   sophisticated clustering and optimization with minimal code and computational
   overhead. Research has shown that simpler approaches often outperform complex
   ones, as demonstrated by Fu and Menzies[1], where simpler learners found
   better configurations more efficiently.

2. Speed & Interactivity: When data analysis is expensive and slow, iterative
   discovery suffers. Interactive exploration, crucial for refining ideas, is
   lost[2]. Waiting hours for cloud computations only to realize a minor tweak
   is needed reminds us of the frustratingly slow batch processing of the 1960s.

3. Explainability & Trust: Simpler systems are inherently easier to understand,
   explain, and audit. Living in a world where critical decisions are made by
   opaque systems that we cannot question or verify is a disquieting prospect.
   Kube.py prioritizes transparency, making its decision processes inspectable
   and understandable.

4. Scientific Integrity: The harder and more expensive it is to run experiments,
   the more challenging reproducibility becomes. This directly impacts the
   trustworthiness of scientific findings. History warns us about relying on
   inadequately scrutinized systems, as seen in failures like the Space Shuttle
   Columbia disaster[3]. Kube's lightweight approach enables rapid
   experimentation and validation.

5. Environmental Sustainability: The energy footprint of "Big AI" is alarming.
   Projections show data center energy requirements doubling, with some AI
   applications already consuming petajoules annually. Such exponential growth
   is simply unsustainable[4].

Menzies's 4th Law: Throw Most Data Away

At the heart of Kube.py is a principle that might seem counterintuitive: For
many tasks in Software Engineering (SE), the best thing to do with most data is
to throw it away. This follows from what Tim Menzies calls his "4th Law" – the
recognition that for many problems, a small subset of the data contains most of
the signal.

This isn't some new fad. These "key" variables have been appearing in AI
research for ages, under different guises: principal components, variable subset
selection, narrows, master variables, and backdoors. It echoes Vilfredo Pareto's
famous 80/20 principle, where 80% of effects come from 20% of causes – though in
practice, the ratio can be even more extreme.

As Menzies has observed, the practical reality is often more dramatic than even
Pareto suggested – not just 80:20 but closer to 1000:1, where a tiny fraction of
factors dictates almost everything in complex systems[5]. This extreme
concentration of influence has been documented across multiple domains:

Historical Evidence

- Amarel's Narrows (1960s): Saul Amarel identified "narrows" in search problems
  – small sets of essential variable settings that, once discovered, drastically
  simplified the search space. His innovation was creating "macros" to jump
  between these narrows, effectively creating shortcuts through complex problem
  spaces[6].

- Variable Pruning (1990s): Kohavi and John demonstrated that removing up to 80%
  of variables in certain datasets still maintained excellent classification
  accuracy, reinforcing the idea that most variables contribute negligible
  information[7].

- ISAMP and Constraint Satisfaction (1990s): Crawford and Baker's ISAMP tool
  used random search with strategic restarts, which proved surprisingly
  effective for constraint satisfaction problems. The key insight was that
  models have a few "master variables" controlling overall behavior, making
  exhaustive searching inefficient[8].

- Backdoors in Satisfiability (2000s): Williams and colleagues found that
  running random searches repeatedly revealed the same few variable settings in
  successful solutions. Setting these "backdoor" variables first made previously
  intractable problems suddenly manageable, providing another demonstration of
  the power of identifying key variables[9].

- Modern Feature Selection and Dimensionality Reduction: Contemporary machine
  learning continues to build on these foundations with techniques like LASSO
  regression (which can reduce coefficients to zero) and Random Forest
  importance metrics that identify the handful of features driving most
  predictions[10].

Empirical Support

Recent work by researchers like Kocaguneli, Tu, Peters, and Xu has shown that
accurate predictions for software engineering tasks like predicting GitHub issue
close times, effort estimation, and defect prediction remained possible even
after discarding labels for 80%, 91%, 97%, and sometimes 98-100% of project
data[11][12][13][14]. This dramatic data reduction without significant
performance loss demonstrates the concentrated nature of information in
real-world datasets.

The mathematical foundation for this phenomenon is also well-established. The
Johnson-Lindenstrauss lemma[15] proves that high-dimensional data can be
projected into significantly lower dimensions while mostly preserving distances
between points. Similarly, research on the "naturalness" of software[16] has
shown that code contains significant repetition and predictable patterns,
explaining why small samples can capture essential behaviors of much larger
systems.

This principle finds further support in studies of software bugs by Ostrand et
al.[17], which demonstrated that typically 80% of defects occur in just 20% of
modules, and in Lin and Robles' work[18] showing how open-source development
follows power laws, where a small fraction of components receive most
development activity.

Menzies et al.[19][20] have demonstrated across multiple studies that static
code attributes can be highly compressed while still maintaining predictive
power for defect models, and that local models built on small subsets of data
can outperform global models trained on everything. The essence of this work is
that more data isn't always better – smartly selected small data often is.


-------------------------------------------------------------------------------

Foundations

Minkowski Distance

Kube uses the Minkowski distance metric as the foundation for measuring
similarity between data points. The formula is:

    d(x, y) = (Σ|x_i - y_i|^P)^(1/P)

Where:
- x and y are two data points
- n is the number of dimensions
- P is the distance formula exponent (default: 2)

When P=2, this becomes the familiar Euclidean distance. Different P values
create different distance spaces, affecting how clusters form.[21]

Locality-Sensitive Hashing (LSH)

Kube implements a form of locality-sensitive hashing for efficient clustering.
LSH works by:

1. Selecting representative "poles" from the data
2. Projecting each data point onto the lines connecting these poles
3. Creating a discrete hash based on these projections
4. Grouping points with identical hashes

This approach has O(n) complexity versus the O(n²) of traditional clustering
methods, making it efficient for large datasets.[22]

Active Learning

The code implements active learning principles by finding the most informative
examples in the data space. The poles() method identifies data points at
maximum distances from each other, effectively sampling the boundaries of the
data space.[23]

Active learning focuses on selecting the most informative examples for labeling,
rather than randomly sampling the data space. This is particularly useful in domains
where labeling is expensive, as it maximizes the information gain from each labeled
example.

Multi-Objective Optimization

Kube optimizes for multiple objectives through the "heaven" concept. For each
column marked with "+" or "-" in the CSV header, kube attempts to maximize or
minimize those values, respectively. The ydist() method calculates how far
each data point is from the ideal point ("heaven").[24]

In multi-objective optimization, there is rarely a single solution that optimizes all
objectives simultaneously. Instead, we seek Pareto-optimal solutions - those where improving
one objective necessarily worsens another. Kube's approach combines multiple objectives into
a single distance metric, allowing for intuitive ranking of solutions.

Dimensionality Reduction

At its core, kube implements a form of dimensionality reduction through projection. This
is related to the Johnson-Lindenstrauss lemma[15], which proves that high-dimensional data can
be projected into significantly lower dimensions while mostly preserving distances between points.

By representing complex, high-dimensional data in a lower-dimensional space, kube makes it
possible to visualize and understand relationships that would otherwise be difficult to perceive.

-------------------------------------------------------------------------------

Techniques in Kube.py

1. Pole Selection: Finding representative points at maximum distance from each
   other, similar to Pearson's principal component analysis (1902) which
   identified that even in datasets with many dimensions, a handful of
   components captured the main "drift" of the data.

2. Locality-Sensitive Hashing: Building on insights from constraint satisfaction
   research that found "random search with retries" surprisingly effective
   because models often have a few "master variables" pulling the strings.

3. Projection: Drawing from the Johnson-Lindenstrauss lemma which demonstrates
   that complex data can often be accurately approximated in lower-dimensional
   spaces while preserving essential relationships.

By focusing on these "key" aspects of data and using computationally efficient
algorithms, Kube.py demonstrates that the future of AI may lie not in attempting
to "boil the ocean" with every query, but in focusing on "small data" with
intelligent, nimble approaches.

These tools enable efficient clustering, optimization, and exploration of
multi-dimensional data spaces.

Pole Selection

The poles() method selects representative data points that span the data space:

    def poles(i) -> Rows:
        """Select poles at max distance to poles picked so far."""
        r0, *some = many(i._rows, k=the.some + 1)
        out = [max(some, key=lambda r1: i.xdist(r1, r0))]
        for _ in range(the.dims):
            out += [max(some, key=lambda r2: sum(i.xdist(r2, r1) for r1 in out))]
        return out
    
This algorithm:
1. Samples the.some random rows
2. Finds the two most distant points
3. Iteratively adds points that maximize the sum of distances to previously
selected poles
4. Continues until the.dims poles are selected

This approach efficiently explores the boundaries of the data space, selecting
points that represent extreme or distinctive characteristics. It's similar to how
principal component analysis identifies dimensions of maximum variance, but uses
a simpler, more intuitive approach.

Projection and Hashing

The project() method projects a data point onto the line connecting two poles:

    def project(i, row: Row, a: Row, b: Row) -> int:
        """Project a row onto the line connecting two poles, a and b"""
        c = i.xdist(a, b)
        x = (i.xdist(row, a)**2 + c**2 - i.xdist(row, b)**2) / (2 * c)
        return min(int(x / c * the.bins), the.bins - 1)
    
This method:
1. Uses the law of cosines to find where a point projects onto a line
2. Calculates the normalized position (0 to 1) along that line
3. Discretizes this position into the.bins bins (default: 5)

The collection of bin assignments across multiple pole pairs forms a unique hash
for clustering. This discretization step is crucial for efficient approximate clustering,
as it allows similar points to share the same hash despite small differences.

Locality-Sensitive Hashing

The lsh() method implements locality-sensitive hashing for efficient clustering:

    def lsh(i, poles: Rows) -> Dict[Tuple[int, ...], 'Data']:
        """Locality sensitive hashing to group rows by projection."""
        clusters: Dict[Tuple[int, ...], 'Data'] = {}
        for row in i._rows:
            k = tuple(i.project(row, a, b) for a, b in zip(poles, poles[1:]))
            clusters[k] = clusters.get(k) or i.clone()
            clusters[k].add(row)
        return clusters

This approach creates efficient approximate clusterings by:
1. Computing projections for each row onto the lines between consecutive poles
2. Using these projections as a hash key
3. Grouping rows with identical hash keys into the same cluster

This method has O(n) complexity versus the O(n²) of traditional clustering methods,
making it efficient for large datasets. It's a form of dimensionality reduction that
preserves local relationships, similar to techniques like t-SNE or UMAP but with much
lower computational cost.

Distance to Heaven

The ydist() method calculates how far a point is from the ideal:

    def ydist(i, row: Row) -> float:
        """Calculate the distance to heaven for this row."""
        return dist(abs(c.norm(row[c.at]) - c.heaven) for c in i.cols.y)
    
This creates a single objective function from multiple objectives by:
1. Normalizing each attribute to a 0-1 scale
2. Measuring how far each attribute is from its ideal value (0 for minimization, 1 for maximization)
3. Combining these distances using the Minkowski distance formula

This approach to multi-objective optimization is elegant in its simplicity.
Rather than using complex Pareto-front calculations, it creates an intuitive distance
metric that can be used to rank solutions. This fits with the minimalist philosophy of kube.

Minimum Points Heuristic

The minPts() method determines how many points are needed for a valid cluster:

    def minPts(i) -> int:
        """Report how many points are needed for each bucket."""
        out = the.min
        if out==0:
          if   len(i._rows) <  30: out= 2
          elif len(i._rows) < 100: out= 3
          else: out = 2 + the.dims
        return out

This heuristic ensures that clusters have enough points to be statistically significant
without being too restrictive. It adapts to the dataset size and dimensionality, making
kube's clustering robust across different applications.

How It All Works Together

The complete clustering process in kube follows these steps:

1. Select representative poles using poles()
2. Project each point onto the lines between consecutive poles using project()
3. Cluster points with identical projections using lsh()
4. Calculate statistics for each cluster
5. Filter clusters to retain only those with at least minPts() points
6. Present results sorted by cluster quality (lowest ydist() first)

This pipeline exemplifies the "small data" philosophy: It efficiently identifies key
structures in the data without requiring exhaustive pairwise comparisons or complex
optimization procedures.

-------------------------------------------------------------------------------

Details: CLI Options and Example Functions

Part 1: CLI Options

Kube.py provides several command-line options to customize its behavior:

Option      Description                     Default     Effect of Increase                               Effect of Decrease
-b bins     Number of bins                  5            More granular clustering, potentially more clusters Coarser clustering, fewer clusters
-m min      Minimum points per cluster      0 (auto)     Larger minimum cluster size, fewer reported clusters Smaller minimum cluster size, more reported clusters
-P P        Distance formula exponent       2            Higher values emphasize larger differences         Lower values give more equal weight to all differences
-d dims     Number of dimensions            4            More clustering dimensions, potentially more precise clusters Fewer dimensions, potentially faster but less precise
-r rseed    Random number seed              1234567891   Different random selections                      Different random selections
-s some     Search space size for poles     30           Larger sample for pole selection, potentially better poles Smaller sample, faster but potentially less optimal poles
-f file     Training CSV file               auto93.csv   N/A (changes input file)                         N/A (changes input file)

Automatic MinPts Selection

When -m is set to 0 (default), kube automatically selects an appropriate
minimum cluster size:
- For datasets < 30 rows: minPts = 2
- For datasets < 100 rows: minPts = 3
- For larger datasets: minPts = 2 + the.dims

This heuristic ensures that clusters have enough points to be statistically significant
without being too restrictive.

Examples of CLI Usage

# Use more bins for finer clustering

  $ python kube.py --counts -b 8

# Use Manhattan distance instead of Euclidean

  $ python kube.py --counts -P 1

# Use more dimensions for clustering

  $ python kube.py --counts -d 6

# Set a fixed minimum cluster size

  $ python kube.py --counts -m 5

# Use a different dataset

  $ python kube.py --counts -f path/to/other/data.csv

Part 2: Example Functions

Kube.py includes several example functions (prefixed with eg__) that
demonstrate different capabilities of the tool. These can be run directly from the
command line:

Function    Command                 Description
eg__all     python kube.py --all    Runs all example functions in sequence
eg__the     python kube.py --the    Prints the current configuration settings
eg__csv     python kube.py --csv    Prints the raw CSV data from the input file
eg__data    python kube.py --data   Prints column information, showing which are independent (x) and dependent (y) variables
eg__ydist   python kube.py --ydist  Prints rows sorted by distance to heaven, showing the 4 best and 4 worst examples
eg__poles   python kube.py --poles  Shows clustering dimensions by displaying the results of pole selection and LSH
eg__counts  python kube.py --counts Shows cluster counts and statistics - the main clustering functionality

Example Functions in Detail

1. eg_h: Help Text

  $ python kube.py -h

This displays the docstring and lists all available example functions.

2. eg__the: Current Configuration

  $ python kube.py --the

  # Output: {b: 5, m: 0, P: 2, d: 4, r: 1234567891, s: 30, f: ../../moot/optimize/misc/auto93.csv}

This is useful for verifying your current settings.

3. eg__csv: Raw Data

  $ python kube.py --csv

  # Output: Shows all rows from the CSV file

This shows the raw input data without any processing.

4. eg__data: Column Information

  $ python kube.py --data

  # Output:
  # x Num(at=0, txt=Clndrs, n=398, mu=5.45, m2=2400.31, lo=3, hi=8, heaven=1)
  # x Num(at=1, txt=Volume, n=398, mu=193.43, m2=1786554.97, lo=68, hi=455, heaven=1)
  # ...
  # y Num(at=5, txt=Lbs-, n=398, mu=2970.42, m2=19431245.76, lo=1613, hi=5140, heaven=0)

This reveals how kube interprets each column (Num or Sym) and whether it's
considered an input (x) or output (y) variable.

5. eg__ydist: Ranking by Distance to Heaven

  $ python kube.py --ydist
  # Output:
  # good [4, 90, 48, 80, 2, 2085, 21.7, 40]
  # good [4, 91, 67, 80, 3, 1850, 13.8, 40]
  # ...
  # bad [8, 455, 225, 70, 1, 4425, 10.0, 10]
  # bad [8, 454, 220, 70, 1, 4354, 9.0, 10]

This shows which data points are closest to optimal (good) and furthest from
optimal (bad).

6. eg__poles: Pole Selection and Clustering
$ python kube.py --poles
# Output:
# (0, 0, 0)
# (0, 1, 1)
# ...
# 14
This shows the hash keys generated for different clusters and the total
number of clusters found.

7. eg__counts: Full Clustering with Statistics
$ python kube.py --counts
# Output:
# {mid: 0.29, div: 0.11, n: 8}
# {mid: 0.45, div: 0.15, n: 12}
# ...
This is the primary analysis function, showing each valid cluster (meeting
minimum size) with its:
- mid: central tendency of y-distances (lower is better)
- div: diversity/spread of the cluster
- n: number of data points in the cluster

-------------------------------------------------------------------------------

Data Model

Kube's data model consists of several key classes designed for efficiency and clarity:

Base Classes

o Class

    class o:
      """Base class providing dictionary update and string representation."""
      __init__ = lambda i, **d: i.__dict__.update(**d)
      __repr__ = cat

This is the foundation class that all other classes inherit from. It provides:
- A simple way to update object attributes with dictionary items
- String representation via the cat function that formats objects for display

    Type Aliases
    # Type aliases
    Atom = Union[int, float, str, bool]
    Row  = List[Atom]
    Rows = List[Row]
    Col  = Union['Sym', 'Num']
    
These type aliases provide clear definitions for the basic data structures:
- Atom: A primitive data type (int, float, str, bool)
- Row: A list of Atoms representing a single data entry
- Rows: A list of Row objects
- Col: Either a Sym or Num column type

Column Classes

    Sym Class
    class Sym(o):
      """Symbol class for handling categorical attributes."""
    
      def __init__(i, has: List[Atom] = [], at: int = 0, txt: str = " "):
        """Initialize a symbol column."""
        i.at: int = at            # Column position
        i.txt: str = txt          # Column name
        i.n: int = 0              # Count of items seen
        i.has: Dict[Atom, int] = {}  # Frequency counts of values
        [i.add(x) for x in has]

The Sym class handles categorical attributes with methods for:
- Frequency counting (has dictionary)
- Distance calculation between symbols
- Finding most common value (mid())
- Calculating entropy as diversity measure (div())

    Num Class
    class Num(o):
      """Number class for handling numeric attributes."""
      def __init__(i, has: List[Atom] = [], at: int = 0, txt: str = " "):
        """Initialize a numeric column."""
        i.at: int = at            # Column position
        i.txt: str = txt          # Column name
        i.n: int = 0              # Count of items seen
        i.mu: float = 0           # Mean of values
        i.m2: float = 0           # Sum of squared differences from mean
        i.lo: float = BIG         # Lowest value seen
        i.hi: float = -BIG        # Highest value seen
        i.heaven: int = 0 if txt[-1] == "-" else 1  # Optimization goal (0=min, 1=max)
        [i.add(x) for x in has]

The Num class handles numerical attributes with methods for:
- Online calculation of mean and standard deviation
- Normalization to 0-1 range
- Distance calculation between numbers
- Optimization direction via heaven attribute (0=minimize, 1=maximize)
Note the clever use of the column name suffix ("-" or "+") to determine the optimization
direction via the heaven attribute.

Data Container

Data Class

    class Data(o):
      """Data class for handling collections of rows."""
      def __init__(i, src: Iterator[Row]):
        """Initialize from data source."""
        i._rows: Rows = []             # Storage for data rows
        i.cols = o(x=[], y=[], all=[]) # Track columns (x=independent, y=dependent, all=all)
        src = iter(src)
        [i.about(c, s) for c, s in enumerate(next(src))]
        [i.add(row) for row in src]

The Data class is the main container for rows and columns with methods for:
- Parsing CSV files
- Separating columns into independent (x) and dependent (y) variables
- Finding central tendency rows
- Calculating distances between rows
- Implementing cluster analysis via lsh() method

Design Philosophy

Kube's data model is designed with several key principles in mind:

1. Minimalism: Each class does one thing well, with a minimum of methods and attributes.
2. Self-documenting: Code is written to be readable and understandable, with clear variable names.
3. Type safety: Python type hints provide documentation and enable static type checking.
4. Online algorithms: Statistical calculations like mean and standard deviation use online
algorithms that don't require storing all data points.
5. Functional style: Many operations use functional programming idioms like list comprehensions.

-------------------------------------------------------------------------------

References

1. Fu, W., & Menzies, T. (2017). Easy over hard: A case study on deep learning. European Software Engineering Conference, 295-306.
2. Fisher, D., DeLine, R., Czerwinski, M., & Drucker, S. (2012). Interactions with big data analytics. Interactions, 19(3), 50-59.
3. Columbia Accident Investigation Board. (2003). Report of the Columbia Accident Investigation Board, Vol. 1. Government Printing Office, Washington, D.C.
4. Strubell, E., Ganesh, A., & McCallum, A. (2019). Energy and policy considerations for deep learning in NLP. Annual Meeting of the Association for Computational Linguistics, 3645-3650.
5. Menzies, T., & Zimmermann, T. (2018). Software analytics: What's next? IEEE Software, 35(5), 64-70.
6. Amarel, S. (1968). On representations of problems of reasoning about actions. Machine Intelligence, 3, 131-171.
7. Kohavi, R., & John, G. H. (1997). Wrappers for feature subset selection. Artificial Intelligence, 97(1-2), 273-324.
8. Crawford, J., & Baker, A. (1994). Experimental results on the application of satisfiability algorithms to scheduling problems. AAAI Conference on Artificial Intelligence, 1092-1097.
9. Williams, R., Gomes, C. P., & Selman, B. (2003). Backdoors to typical case complexity. International Joint Conference on Artificial Intelligence, 1173-1178.
10. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B, 58(1), 267-288.
11. Kocaguneli, E., Menzies, T., Bener, A., & Keung, J. W. (2013). Exploiting the essential assumptions of analogy-based effort estimation. IEEE Transactions on Software Engineering, 39(7), 957-972.
12. Tu, H., Menzies, T., & He, B. (2022). Data to the people: Lazy learning, data reduction, simplified reasoning. Automated Software Engineering, 29(1), 1-30.
13. Peters, F., Menzies, T., & Marcus, A. (2013). Better cross company defect prediction. IEEE International Working Conference on Mining Software Repositories, 409-418.
14. Xu, B., Xing, Z., Xia, X., & Lo, D. (2021). AnswerBot: automated generation of answer summary to developers' technical questions. IEEE Transactions on Software Engineering, 47(5), 998-1015.
15. Johnson, W. B., & Lindenstrauss, J. (1984). Extensions of Lipschitz mappings into a Hilbert space. Contemporary Mathematics, 26, 189-206.
16. Hindle, A., Barr, E. T., Su, Z., Gabel, M., & Devanbu, P. (2012). On the naturalness of software. International Conference on Software Engineering, 837-847.
17. Ostrand, T. J., Weyuker, E. J., & Bell, R. M. (2004). Where the bugs are. ACM SIGSOFT Software Engineering Notes, 29(4), 86-96.
18. Lin, B., & Robles, G. (2015). Power laws in FLOSS development. Journal of Systems and Software, 107, 85-99.
19. Menzies, T., Greenwald, J., & Frank, A. (2007). Data mining static code attributes to learn defect predictors. IEEE Transactions on Software Engineering, 33(1), 2-13.
20. Menzies, T., Butcher, A., Marcus, A., Zimmermann, T., & Cok, D. (2011). Local vs. global models for effort estimation and defect prediction. IEEE International Conference on Automated Software Engineering, 343-351.
21. Cha, S. H. (2007). Comprehensive survey on distance/similarity measures between probability density functions. International Journal of Mathematical Models and Methods in Applied Sciences, 1(4), 300-307.
22. Gionis, A., Indyk, P., & Motwani, R. (1999). Similarity search in high dimensions via hashing. VLDB, 99(6), 518-529.
23. Settles, B. (2009). Active learning literature survey. University of Wisconsin-Madison Department of Computer Sciences.
24. Deb, K. (2011). Multi-objective optimization using evolutionary algorithms: an introduction. Multi-objective evolutionary optimisation for product design and manufacturing, 1-24.
25. Pearson, K. (1901). On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11), 559-572.
26. Faloutsos, C., & Lin, K. (1995). FastMap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets. ACM SIGMOD Record, 24(2), 163-174.
27. Menzies, T., Kocagüneli, E., Minku, L., Peters, F., & Turhan, B. (2013). The PROMISE Repository of empirical software engineering data. West Virginia University, Department of Computer Science.
28. Nair, V., Menzies, T., Siegmund, N., & Apel, S. (2018). Using bad learners to find good configurations. European Software Engineering Conference, 257-269.
29. Xu, Z., Krushevskaja, D., Ley-Wild, R., & Xie, T. (2015). Gotcha: An interactive debugger for missing cache invalidations. IEEE Transactions on Software Engineering, 42(10), 940-958.
30. Hutson, M. (2018). Artificial intelligence faces reproducibility crisis. Science, 359(6377), 725-726.

-------------------------------------------------------------------------------

Credits and Acknowledgments

Author
Kube.py was created by Tim Menzies (timm@ieee.org), a Professor
of Computer Science at North Carolina State University and a leading researcher in the field
of software analytics and search-based software engineering.

Copyright (c) 2025 Tim Menzies

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

References

For a complete list of references, please see the References page.

Contributing

Contributions to kube.py are welcome! Please fork the repository, make your changes, and submit a pull request.

Key areas for future work include:
- Additional dimensionality reduction techniques
- Improved visualization tools
- More sophisticated active learning strategies
- Applications to new domains

Mathematical and AI Foundations
© 2025 Tim Menzies. MIT License.
